{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "####################CLIFF WALKING ENVIRONMENT#########################\n",
    "\n",
    "A schematic view of the environment-\n",
    "\n",
    "o  o  o  o  o  o  o  o  o  o  o  o\n",
    "o  o  o  o  o  o  o  o  o  o  o  o\n",
    "o  o  o  o  o  o  o  o  o  o  o  o\n",
    "S  x  x  x  x  x  x  x  x  x  x  T\n",
    "\n",
    "Actions: \n",
    "    UP (0)\n",
    "    DOWN (1)\n",
    "    RIGHT (2)\n",
    "    LEFT (3)\n",
    "\n",
    "Rewards: \n",
    "     0 for going in Terminal state\n",
    "    -100 for falling in the cliff\n",
    "    -1 for all other actions in any state\n",
    "\n",
    "Note: State remains the same on going out of the maze (but -1 reward is given)\n",
    "      The episode ends and the agent returns to the start state after falling in the cliff\n",
    "\n",
    "'''\n",
    "START_STATE = 36\n",
    "TERMINAL_STATE = 47\n",
    "def reward(state):\n",
    "    if(state == TERMINAL_STATE):\n",
    "        reward = 0\n",
    "    elif(state > START_STATE and state < TERMINAL_STATE):\n",
    "        reward = -100\n",
    "    else:\n",
    "        reward = -1\n",
    "    return reward\n",
    "\n",
    "def env(state, action):\n",
    "    # return_val = [prob, next state, reward, isdone]\n",
    "    num_states = rows * columns\n",
    "    isdone = lambda state: state > START_STATE and state <= TERMINAL_STATE\n",
    "    \n",
    "    if(isdone(state)):\n",
    "        next_state = state\n",
    "    else:\n",
    "        if(action==0):\n",
    "            next_state = state-columns if state-columns>=0 else state\n",
    "        elif(action==1):\n",
    "            next_state = state+columns if state+columns<num_states else state\n",
    "        elif(action==2):\n",
    "            next_state = state+1 if (state+1)%columns else state\n",
    "        elif(action==3):\n",
    "            next_state = state-1 if state%columns else state \n",
    "    # State Transition Probability is 1 because the environment is deterministic\n",
    "    return_val = [1, next_state, reward(next_state), isdone(next_state)]\n",
    "    return return_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1 # Learning Rate\n",
    "epsilon = 0.1 # For Epsilon-greedy policy to balance exploration and exploitation\n",
    "rows = 4\n",
    "columns = 12\n",
    "num_states = rows * columns\n",
    "num_actions = 4\n",
    "gamma = 1 # Discount Factor\n",
    "episodes = 100000 # Number of games played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa():\n",
    "    # Initialize the action value function\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    for episode in range(episodes):\n",
    "        # Initialize S\n",
    "        curr_state = START_STATE\n",
    "        # Pick a random number between 0 and 1\n",
    "        P = np.random.random()\n",
    "        if(P > epsilon):\n",
    "            # Pick the greedy action\n",
    "            curr_action = np.argmax(Q[curr_state])\n",
    "        else:\n",
    "            # Pick a random action to explore\n",
    "            curr_action = np.random.randint(0, num_actions)\n",
    "        while True:\n",
    "            # prob: State Transition Probability \n",
    "            # reward, next_state: Immediate reward and next state on taking curr_action in curr_state\n",
    "            # isdone: Whether the next state is Terminal or not\n",
    "            prob, next_state, reward, isdone = env(curr_state, curr_action)\n",
    "            # Pick a random number between 0 and 1\n",
    "            P = np.random.random()\n",
    "            if(P > epsilon):\n",
    "                # Pick the greedy action\n",
    "                next_action = np.argmax(Q[next_state])\n",
    "            else:\n",
    "                # Pick a random action to explore\n",
    "                next_action = np.random.randint(0, num_actions)\n",
    "            # Update the current state-action value\n",
    "            Q[curr_state, curr_action] += alpha * (reward + gamma * Q[next_state, next_action] - Q[curr_state, curr_action])\n",
    "            curr_state = next_state\n",
    "            curr_action = next_action\n",
    "            if isdone:\n",
    "                break\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function for SARSA:\n",
      " [[ -15.94559988  -16.9948888   -14.69603306  -15.72345508]\n",
      " [ -15.05465572  -15.76705586  -13.54840985  -15.87710741]\n",
      " [ -13.45242819  -14.69868729  -12.16853176  -14.94160782]\n",
      " [ -12.63413295  -13.699749    -10.96799573  -13.63458863]\n",
      " [ -11.61395304  -13.18976409  -10.12519889  -12.66692852]\n",
      " [ -10.21356605  -11.41455096   -8.81595204  -11.54811983]\n",
      " [  -8.89894462   -9.44245617   -7.84409062  -10.38606989]\n",
      " [  -7.97057786   -7.50709271   -6.86752574   -9.04047756]\n",
      " [  -6.71601565   -6.28608529   -5.76857999   -8.10824048]\n",
      " [  -5.75416482   -6.05996278   -4.48956925   -6.89134512]\n",
      " [  -4.53879864   -3.37399566   -3.69865185   -5.72865438]\n",
      " [  -3.29939681   -2.29040333   -3.38761926   -4.6112411 ]\n",
      " [ -15.77520247  -18.15055322  -16.08927591  -16.81982097]\n",
      " [ -14.82251886  -20.91290962  -15.2901838   -17.36156562]\n",
      " [ -13.41420048  -19.27629725  -13.80332536  -15.83643976]\n",
      " [ -12.2886318   -14.46490268  -13.37879625  -14.82369556]\n",
      " [ -11.34638929  -18.07730175  -12.99867283  -13.01454097]\n",
      " [ -10.05664622  -20.45599566  -10.73889774  -11.73584902]\n",
      " [  -9.03976897  -16.21977817   -7.58169294  -10.58614222]\n",
      " [  -7.93519245   -8.3894575    -6.01766      -8.55182664]\n",
      " [  -6.85737218   -7.28799262   -4.91850924   -7.70474178]\n",
      " [  -5.5690163   -10.53676369   -3.66083772   -6.42503408]\n",
      " [  -4.69863053   -3.32842077   -2.19363753   -4.66993645]\n",
      " [  -3.54830167   -1.03148761   -2.19113083   -3.50378001]\n",
      " [ -16.76027556  -23.30124192  -19.54379671  -18.37898227]\n",
      " [ -15.6939623   -99.99994615  -19.62986193  -17.61662049]\n",
      " [ -14.68592049  -98.66972054  -19.19575655  -15.8211625 ]\n",
      " [ -13.55443885  -98.17519964  -15.045836    -14.57543324]\n",
      " [ -12.14161168  -95.76088417  -18.81824128  -13.62541004]\n",
      " [ -12.42065446  -93.53891811  -12.56506205  -12.42885287]\n",
      " [  -8.64001917  -94.76652367  -10.40188297  -11.07638881]\n",
      " [  -7.54225118  -93.53891811  -11.28587451   -9.62520034]\n",
      " [  -6.03266808  -98.35767967  -11.51128787   -8.70045731]\n",
      " [  -4.6098461   -97.97244404   -8.87336206   -9.30967013]\n",
      " [  -3.42441529  -99.99989866   -1.01445727  -13.83482973]\n",
      " [  -2.58604241    0.           -1.11134146   -2.48612962]\n",
      " [ -17.78781573  -24.29407585 -100.          -19.33788316]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]]\n",
      "\n",
      "\n",
      "Deterministic Policy:\n",
      " [[2 2 2 2 2 2 2 2 2 2 1 1]\n",
      " [0 0 0 0 0 0 2 2 2 2 2 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 2 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "Q = sarsa()\n",
    "# Deterministic policy obtained using updated Q values\n",
    "policy = np.argmax(Q,axis=1)\n",
    "print(f'Value Function for SARSA:\\n {Q.reshape(num_states, num_actions)}')\n",
    "print('\\n')\n",
    "print(f'Deterministic Policy:\\n {policy.reshape(rows, columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict = {0:'Up', 1:'Down', 2:'Right', 3:'Left'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearning():\n",
    "    # Initialize the action value function\n",
    "    Q = np.zeros((num_states, num_actions))\n",
    "    for episode in range(episodes):\n",
    "        # Initialize S\n",
    "        curr_state = START_STATE\n",
    "        while True:\n",
    "            # Generate a random number between 0 and 1\n",
    "            P = np.random.random()\n",
    "            if(P > epsilon):\n",
    "                # Pick the greedy action\n",
    "                curr_action = np.argmax(Q[curr_state])\n",
    "            else:\n",
    "                # Pick a random action to explore\n",
    "                curr_action = np.random.randint(0, num_actions)\n",
    "            # prob: State Transition Probability \n",
    "            # reward, next_state: Immediate reward and next state on taking curr_action in curr_state\n",
    "            # isdone: Whether the next state is Terminal or not    \n",
    "            prob, next_state, reward, isdone = env(curr_state, curr_action)\n",
    "            # Update the current state-action value\n",
    "            Q[curr_state, curr_action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[curr_state, curr_action])\n",
    "            curr_state = next_state\n",
    "            if isdone:\n",
    "                break\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function:\n",
      " [[ -12.98693292  -12.87869316  -12.88008789  -12.93179274]\n",
      " [ -12.06615657  -11.96851442  -11.96921728  -11.98307952]\n",
      " [ -11.09393501  -10.99036754  -10.98978787  -11.26899692]\n",
      " [ -10.15498083   -9.99675241   -9.99668805  -10.6014942 ]\n",
      " [  -9.30616091   -8.99841075   -8.99845141   -9.73112232]\n",
      " [  -8.43465085   -7.99926849   -7.99926466   -8.13298933]\n",
      " [  -7.37140155   -6.99967301   -6.99963809   -8.10063723]\n",
      " [  -6.38793972   -5.99984205   -5.99982477   -6.65121566]\n",
      " [  -5.32798129   -4.99993813   -4.99993187   -5.71859   ]\n",
      " [  -4.47703275   -3.99997724   -3.9999785    -4.93731032]\n",
      " [  -3.50419978   -2.99999506   -2.99999465   -3.8717001 ]\n",
      " [  -2.52114681   -2.           -2.26217078   -2.49145936]\n",
      " [ -13.79299651  -12.          -12.          -12.99977415]\n",
      " [ -12.92682326  -11.          -11.          -12.99953489]\n",
      " [ -11.96857313  -10.          -10.          -11.99874289]\n",
      " [ -10.98998707   -9.           -9.          -10.99791568]\n",
      " [  -9.99231524   -8.           -8.           -9.99938643]\n",
      " [  -8.99503324   -7.           -7.           -8.99691114]\n",
      " [  -7.99742256   -6.           -6.           -7.99803546]\n",
      " [  -6.99162309   -5.           -5.           -6.99720092]\n",
      " [  -5.99787513   -4.           -4.           -5.99768966]\n",
      " [  -4.99935282   -3.           -3.           -4.99854004]\n",
      " [  -3.99659178   -2.           -2.           -3.99816337]\n",
      " [  -2.99663517   -1.           -1.99962982   -2.99339613]\n",
      " [ -13.          -13.          -11.          -12.        ]\n",
      " [ -12.         -100.          -10.          -12.        ]\n",
      " [ -11.         -100.           -9.          -11.        ]\n",
      " [ -10.         -100.           -8.          -10.        ]\n",
      " [  -9.         -100.           -7.           -9.        ]\n",
      " [  -8.         -100.           -6.           -8.        ]\n",
      " [  -7.         -100.           -5.           -7.        ]\n",
      " [  -6.         -100.           -4.           -6.        ]\n",
      " [  -5.         -100.           -3.           -5.        ]\n",
      " [  -4.         -100.           -2.           -4.        ]\n",
      " [  -3.         -100.           -1.           -3.        ]\n",
      " [  -2.            0.           -1.           -2.        ]\n",
      " [ -12.          -13.         -100.          -13.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]\n",
      " [   0.            0.            0.            0.        ]]\n",
      "\n",
      "\n",
      "Deterministic Policy:\n",
      " [[1 1 2 2 1 2 2 2 2 1 2 1]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1]\n",
      " [2 2 2 2 2 2 2 2 2 2 2 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "Q = qlearning()\n",
    "# Deterministic policy obtained using updated Q values\n",
    "policy = np.argmax(Q,axis=1)\n",
    "print(f'Value Function:\\n {Q.reshape(num_states, num_actions)}')\n",
    "print('\\n')\n",
    "print(f'Deterministic Policy:\\n {policy.reshape(rows, columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dict = {0:'Up', 1:'Down', 2:'Right', 3:'Left'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA takes a safer path than Q-learning, which takes the optimal path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decrease the value of epsilon (exploration rate)\n",
    "epsilon = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function for SARSA:\n",
      " [[-12.11387557 -12.04449439 -12.01281371 -11.97263809]\n",
      " [-11.41488169 -11.47062249 -11.42876236 -11.45544762]\n",
      " [-10.65272594 -10.60482184 -10.58601944 -10.67164215]\n",
      " [ -9.78173365  -9.73059159  -9.70831792  -9.7395596 ]\n",
      " [ -8.82396973  -8.79631318  -8.79268092  -8.9510577 ]\n",
      " [ -7.86074961  -7.86721059  -7.86580772  -7.87363929]\n",
      " [ -7.01043849  -6.9268976   -6.92134178  -6.97949329]\n",
      " [ -6.11786885  -5.95194311  -5.95013869  -6.05900709]\n",
      " [ -4.97780293  -4.97335312  -4.97042045  -5.1226547 ]\n",
      " [ -4.01577536  -3.98903846  -3.98838761  -4.06850761]\n",
      " [ -3.06045075  -2.99827569  -2.99826478  -3.01364584]\n",
      " [ -2.09632904  -2.00039369  -2.1057925   -2.01546667]\n",
      " [-12.64192219 -14.08995187 -12.00769142 -12.94906929]\n",
      " [-12.17237884 -12.89960596 -11.00420622 -13.01766285]\n",
      " [-11.2010063  -11.73254742 -10.00298998 -11.82863142]\n",
      " [-10.2705465  -10.89222334  -9.00454018 -11.09299558]\n",
      " [ -9.4881521   -8.4112217   -8.00889838  -9.95036188]\n",
      " [ -8.3350533   -7.44934432  -7.01443534  -8.76217187]\n",
      " [ -7.73679133  -6.30700019  -6.01693677  -8.06506794]\n",
      " [ -6.74043872  -5.38674728  -5.01270258  -7.02391023]\n",
      " [ -5.33769822  -4.35185203  -4.00457293  -5.70210649]\n",
      " [ -4.69361929  -3.76910479  -3.          -4.91041672]\n",
      " [ -3.83643919  -2.21883798  -2.          -3.99741283]\n",
      " [ -2.73711677  -1.          -1.97118064  -3.12310459]\n",
      " [-13.01265166 -14.88007033 -13.81098646 -13.87790059]\n",
      " [-12.01299881 -34.39       -13.31199448 -12.30747861]\n",
      " [-11.00498626 -34.39       -11.07402037 -11.12246991]\n",
      " [-10.00748128 -27.1        -10.4651     -10.31245259]\n",
      " [ -7.63496801 -40.951       -7.38050925  -7.74265897]\n",
      " [ -6.91256686 -10.          -6.53931103  -6.91507208]\n",
      " [ -5.99691901 -19.          -6.04152516  -6.03292245]\n",
      " [ -5.48691928 -19.          -5.4773566   -5.4618051 ]\n",
      " [ -4.79926537 -27.1         -4.80043917  -4.93829197]\n",
      " [ -3.96233553 -34.39        -4.61        -4.07681377]\n",
      " [ -1.39044591 -34.39        -1.          -1.60442682]\n",
      " [ -1.69524334   0.          -0.90152291  -1.75913726]\n",
      " [-14.01784321 -14.89955267 -89.05810109 -14.89503522]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]]\n",
      "\n",
      "\n",
      "Deterministic Policy:\n",
      " [[3 0 2 2 2 0 2 2 2 2 2 1]\n",
      " [2 2 2 2 2 2 2 2 2 2 2 1]\n",
      " [0 0 0 0 2 2 0 3 0 0 2 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "Q = sarsa()\n",
    "# Deterministic policy obtained using updated Q values\n",
    "policy = np.argmax(Q,axis=1)\n",
    "print(f'Value Function for SARSA:\\n {Q.reshape(num_states, num_actions)}')\n",
    "print('\\n')\n",
    "print(f'Deterministic Policy:\\n {policy.reshape(rows, columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA becomes less conservative on decreasing exploration rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
